{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJzC_sZHo4qY"
      },
      "source": [
        "# Assignment 8: Deep Q-Networks\n",
        "---\n",
        "\n",
        "In this assignment, we will solve a classic control problem called Cartpole using OpenAI Gym ([link](https://gym.openai.com/envs/CartPole-v1/)). In particular, we will using the DQN algorithm ([Link](https://openai.com/blog/openai-baselines-dqn/)) to solve the problem.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152690560-12ee45f2-69bb-422c-b230-90e4430f15b3.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Performance of an untrained agent on Cartpole.</em>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152689694-f72560fd-cbc1-4f55-87fc-c5e21655d667.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Performance of an agent during training.</em>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDR8bJ6g5co8"
      },
      "source": [
        "###The CartPole-v1 Evironment \n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152835304-fa8af20e-d6c5-4b41-b36e-21b1ebe66240.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: RL pipeline. </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "First, let's get familiar with the terminology of the CartPole-v1 environment.\n",
        "\n",
        "\n",
        "__Actions:__\n",
        "  - 0 = Push cart to the left\n",
        "  - 1 =\tPush cart to the right \n",
        "\n",
        "__Observations (states):__\n",
        "  - Cart Position \n",
        "  - Cart Velocity \n",
        "  - Pole Angle \n",
        "  - Pole Velocity at Tip\n",
        "\n",
        "__Reward:__ \n",
        "  - For each timestep that the agent is \"not fall,\" it collects a  reward of `+1`\n",
        "\n",
        "__Other terminology:__\n",
        "\n",
        "- Step = agent taking one action in the environment\n",
        "- Done = flag of the termination of the current episode\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's import necessary packages/libraries and specify the environment. \n",
        "\n",
        "                    import time\n",
        "                    import gym\n",
        "                    import random\n",
        "                    import numpy as np\n",
        "                    from collections import deque\n",
        "\n",
        "                    import tensorflow as tf\n",
        "                    from tensorflow import keras\n",
        "                    from keras.models import Sequential\n",
        "                    from keras.layers import Dense\n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "Next, let's check the values of some variable.  \n",
        "\n",
        "                    random_action = env.action_space.sample()\n",
        "                    env.reset()\n",
        "                    observation, reward, done, info = env.step(random_action)\n",
        "\n",
        "                    print(f\"Action = {random_action}\")\n",
        "                    print(f\"Observation = {observation}, shape = {observation.shape}\")\n",
        "                    print(f\"Reward = {reward}\")\n",
        "                    print(f\"Done = {done}\")\n",
        "\n",
        "                    print(f\"Number of actions that can be taken = {env.action_space.n}\")\n",
        "                    print(f\"Limits of the observation: \\n max ={env.observation_space.low} \\n min ={env.observation_space.high}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqntPkYR5TPc"
      },
      "source": [
        "Next, let's implement two helper functions. \n",
        "\n",
        "__1. select_action()__\n",
        "\n",
        "- The agent can choose either the best action learned so far (exploitation) or a random action (exploration) \n",
        "- The exploration rate, which linearly decays from 50% to 2.5%, is used to decide the action\n",
        "\n",
        "                    def select_action(min_exploration_rate, current_exploration_rate, observation, dqn_agent):\n",
        "                      rand_num = random.random() \n",
        "                      if rand_num <= current_exploration_rate:\n",
        "                        action = 0 if current_exploration_rate <= min_exploration_rate/2 else 1 \n",
        "                      else: \n",
        "                        action = np.argmax(dqn_agent.predict(observation)[0])\n",
        "                      return action "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr4YJ-AkHgFK"
      },
      "source": [
        "__2. replay()__\n",
        "\n",
        "  - The two main features of DQN are the following:\n",
        "    - Target network: there are two copies of the agent, namely policy network and target network. During training, the target network gets a copy of the policy network every N episodes (set to 10 in this assignment). \n",
        "\n",
        "    - Experience replay: store transitions that the agent has taken so far; every episode samples a batch of these transitions to train the agent.\n",
        "\n",
        "\n",
        "- For every trajectory sampled, a target Q-value is the immediate reward of taking action `a` in state `s` plus the discounted max `Q` value among all possible actions from the successor state `s'`.  The `Q` in the equation below represents the target network. \n",
        "\n",
        "<br>\n",
        "\n",
        "$$Q(s,a) = r(s,a) + \\gamma \\cdot max_{a} Q(s', a)~~..........~~(Equation~1)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "## Exercise 1\n",
        "\n",
        "- In the code snippet below, please implement Equation 1.\n",
        "\n",
        "- Hints: \n",
        "    - $r(s,a) = reward$\n",
        "    - $\\gamma$ is taken as an agrument (gamma) in the function\n",
        "    - For max operation use the np.amax() from numpy ([link](https://numpy.org/doc/stable/reference/generated/numpy.amax.html))\n",
        "    - Use `target_agent.predict(observation_next)[0]` to get the Q value\n",
        "\n",
        "                    def replay(memory, dqn_agent, target_agent, batch_size, gamma):\n",
        "                      states_batch = [] \n",
        "                      q_values_batch = []\n",
        "\n",
        "                      minibatch = random.sample(memory, batch_size)\n",
        "                      for current_observation,action, reward, observation_next, done in minibatch: \n",
        "                        if not done:\n",
        "                          target = ############ YOUR CODE HERE ###############\n",
        "                        else: \n",
        "                          target = reward \n",
        "\n",
        "                        current_q = dqn_agent.predict(current_observation)\n",
        "                        current_q[0][action] = target \n",
        "\n",
        "                        states_batch.append(current_observation[0])\n",
        "                        q_values_batch.append(current_q[0]) \n",
        "\n",
        "                      states_batch = np.array(states_batch)\n",
        "                      q_values_batch = np.array(q_values_batch)\n",
        "                      dqn_agent.fit(states_batch, q_values_batch, epochs=1, verbose=0) \n",
        "                      return dqn_agent "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to set some learning parameters. \n",
        "                \n",
        "                    max_episodes = 75 \n",
        "                    gamma = 0.97 \n",
        "                    memory = deque(maxlen=256)\n",
        "                    target_update_steps = 10\n",
        "\n",
        "                    num_actions = env.action_space.n\n",
        "                    print(f\"Number of actions that can be taken = {num_actions}\")\n",
        "                    num_observations = env.observation_space.shape[0]\n",
        "\n",
        "                    batch_size = 64\n",
        "                    learning_rate = 1e-4 #0.0001\n",
        "                    Adam = keras.optimizers.Adam "
      ],
      "metadata": {
        "id": "r2vYDVQIUOej"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qCnCX771LE9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Exercise 2\n",
        "\n",
        "Our DQN agent uses a 4-layer neural network, which takes the observation as input and output an action for the agent to execute. \n",
        "\n",
        "Let's initialize a Sequential model: \n",
        "\n",
        "                  dqn_agent = Sequential()\n",
        "\n",
        "Using `dqn_agent.add()` function to build a DQN agent with the following architecture.\n",
        "  - Dense layer with `64` output units and `relu` activation, remember to specity `input_dim`\n",
        "  - Dense layer with `64` output units and `relu` activation\n",
        "  - Dense layer with `24` output units and `relu` activation\n",
        "  - Dense layer with `num_actions` output units and `linear` activation\n",
        "\n",
        "- Compile the `dqn_agent` with `mse` loss and `Adam` optimizer (make sure you specify the `learning_rate`)\n",
        "\n",
        "- Print the `dqn_agent` model summary\n",
        "- References \n",
        "  - Dense ([link](https://keras.io/api/layers/core_layers/dense/))\n",
        "  - Compile ([link](https://keras.io/api/models/model_training_apis/))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's set the parameters of the following \"exponentially decaying $\\epsilon$-greedy strategy\": \n",
        "$$\\text{For episode n:}$$\n",
        "$$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\text{current exploration probability(%)}~=max[~ϵ_{min}, ~ϵ_{start}(ϵ_{decay~rate})^n] $$\n",
        "\n",
        "We will start with `100%` exploration and gradually decrease it to `5%`.\n",
        "\n",
        "                  eps_start = 1.0\n",
        "                  eps_decay_rate = 0.95\n",
        "                  eps_min = 0.05"
      ],
      "metadata": {
        "id": "gDnnGTFa1fqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Exercise 3\n",
        "\n",
        "Please fill the missing code in the following main training loop. \n",
        "\n",
        "Hints: \n",
        "\n",
        "1. __Select action:__ make use of the select_action function defined above\n",
        "2. __Take a step in the environment:__ make use of the step() function\n",
        "3. __Gather experience:__ what constitutes a single experience?\n",
        "\n",
        "                    update_counter = 0 \n",
        "                    reward_collected = []\n",
        "                   \n",
        "                    target_agent = dqn_agent \n",
        "                    current_exploration_rate = eps_start \n",
        "                    \n",
        "                    start = time.time()\n",
        "                    for episode in range(max_episodes): \n",
        "\n",
        "                      print(f\"Episode {episode}:\", end = ' ')\n",
        "                      current_observation = env.reset().reshape(1,-1) \n",
        "\n",
        "                      done = False \n",
        "                      score = 0\n",
        "                      timestep = 0\n",
        "\n",
        "                      while not done:\n",
        "                        action = ########## YOUR CODE HERE (1) ##############\n",
        "                       \n",
        "                        observation_next , reward, done, info  = ########## YOUR CODE HERE (2) ##############\n",
        "                        observation_next = observation_next.reshape(1,-1)\n",
        "\n",
        "                        experience = ########## YOUR CODE HERE (3) ##############\n",
        "                        memory.append(experience)\n",
        "\n",
        "                        current_observation = observation_next\n",
        "                        score += reward\n",
        "                        timestep+=1 \n",
        "\n",
        "                      # Out of while loop\n",
        "                      print(f\"Exploration rate = {round(current_exploration_rate,3)},\", end = \"\\t\")\n",
        "                      current_exploration_rate = max(eps_min, current_exploration_rate*eps_decay_rate) \n",
        "                      \n",
        "                      print(f\"completed with {timestep} timesteps, score = {score}\", end = '\\n')\n",
        "                      reward_collected.append(score)\n",
        "\n",
        "                      if len(memory) >= batch_size: \n",
        "                        dqn_agent  = replay(memory, dqn_agent, target_agent, batch_size, gamma)\n",
        "\n",
        "                      update_counter+= 1\n",
        "                      if update_counter% target_update_steps ==0: \n",
        "                        target_agent.set_weights(dqn_agent.get_weights()) \n",
        "\n",
        "                    env.close()\n",
        "                    print(f\"Total training time taken = {time.time() - start} seconds\")"
      ],
      "metadata": {
        "id": "siuiVxroV2Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, use the code snippet below to plot the learning curve. \n",
        "\n",
        "                    fig, ax = plt.subplots(figsize = (8,5))\n",
        "                    x = len(reward_collected)\n",
        "                    ax.plot(range(x), reward_collected)\n",
        "                    ax.set_title(f\"Reward Collected over {x} episodes\")\n",
        "                    ax.set_xlabel(\"Episodes\")\n",
        "                    ax.set_ylabel(\"Reward per episode\")\n",
        "                    ax.set_xticks(range(0,x+1,10));\n",
        "                    \n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153685218-326ad9a1-e6f9-49a8-aab0-358e76bf236e.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 4: Reward plot (Your plot may look different) </em>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "VsaARmMb4zHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The increase of `reward collected per episode` as the number of episodes progresses implies the agent is learning. \n",
        "\n",
        "Now, use the code below to render a rollout of agent performing the task.\n",
        "\n",
        "Package installation:\n",
        "\n",
        "                    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "                    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "                    !pip install colabgymrender==1.0.2\n",
        "\n",
        "Rendering code:\n",
        "\n",
        "                    from colabgymrender.recorder import Recorder \n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "                    env = Recorder(env, './video')\n",
        "\n",
        "                    observation = env.reset().reshape(1,-1)\n",
        "                    done = False\n",
        "                    while not done:\n",
        "                      learned_action = np.argmax(dqn_agent.predict(observation)[0])\n",
        "                      observation, reward, done, info = env.step(learned_action)  \n",
        "                      observation = observation.reshape(1,-1)\n",
        "                    env.play()"
      ],
      "metadata": {
        "id": "zaXDkia1TpTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: \n",
        "\n",
        "Since fully training the agent can take a while. You can instead terminate the training cell above and load an agent that has been trained for 1000 episodes.\n",
        "\n",
        "Load the agent:\n",
        "\n",
        "                    !wget -o -q https://github.com/poudel-bibek/Intro-to-AI-Assignments/files/8027697/my_agent.zip\n",
        "                    !unzip -o -q ./my_agent.zip -d unzipped/ \n",
        "                    trained_agent = tf.keras.models.load_model('./unzipped/my_agent')\n",
        "\n",
        "Using the agent: \n",
        "\n",
        "                    env = gym.make(\"CartPole-v1\")\n",
        "                    env = Recorder(env, './video')\n",
        "\n",
        "                    observation = env.reset().reshape(1,-1)\n",
        "                    done = False\n",
        "                    while not done:\n",
        "                      # Get best action from agent \n",
        "                      learned_action = np.argmax(trained_agent.predict(observation)[0])\n",
        "\n",
        "                      # Apply the best action in the environment\n",
        "                      observation, reward, done, info = env.step(learned_action)  \n",
        "                      observation = observation.reshape(1,-1)\n",
        "                    env.play()"
      ],
      "metadata": {
        "id": "G7XQIWHPZFLm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}