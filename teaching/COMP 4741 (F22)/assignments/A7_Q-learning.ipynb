{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APLXXQhRu17g"
      },
      "source": [
        "# Assignment 7: Q-Learning\n",
        "---\n",
        "In this assignment, we will solve a classic control problem called Mountain Car using OpenAI Gym ([link](https://gym.openai.com/envs/MountainCar-v0/)). The goal is to drive the car to reach the top of the hill marked by a yellow flag. To achieve that, we need to steer the car back and forth to gain enough momentum. We will use Q-Learning algorithm ([Link](https://en.wikipedia.org/wiki/Q-learning)) to solve the task.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153467131-a630edb2-aeb9-4694-bd69-28382b49c25a.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Performance of an untrained car.</em>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153467239-0e681df2-0f79-4453-81e0-b2ac1e9fd378.gif\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Performance of a trained car.</em>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiWZfywPujuk"
      },
      "source": [
        "## The MountainCar-v0 Environment \n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/152835304-fa8af20e-d6c5-4b41-b36e-21b1ebe66240.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: RL pipeline. </em>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "First, let's get familiar with the terminology of the MountainCar-v0 environment.\n",
        "\n",
        "\n",
        "\n",
        "__Actions:__ \n",
        "  - 0 =\tpush left\n",
        "  - 1 =\tno push\n",
        "  - 2 =\tpush right\n",
        "\n",
        "__Observations (states):__ \n",
        "  - Position\n",
        "  - Velocity\n",
        "\n",
        "__Reward:__ \n",
        "  - `-1` for each timestep the agent spends in the environment until the termination of an episode\n",
        "  - The goal of the agent is to solve the task as quickly as possible\n",
        "  \n",
        "__Other terminology:__\n",
        "\n",
        "- Step = agent taking one action in the environment\n",
        "- Done = flag of the termination of the current episode\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's import necessary packages/libraries and specify the environment. \n",
        "                    \n",
        "                    import time\n",
        "                    import random\n",
        "                    import numpy as np \n",
        "                    import matplotlib.pyplot as plt \n",
        "                    import gym \n",
        "                    env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "\n",
        "Next, let's check the values of some variable. \n",
        "\n",
        "                    random_action = env.action_space.sample() \n",
        "                    env.reset() \n",
        "                    observation, reward, done, info = env.step(random_action)\n",
        "\n",
        "                    print(f\"Action = {random_action}\")\n",
        "                    print(f\"Observation = {observation}, shape = {observation.shape}\")\n",
        "                    print(f\"Reward = {reward}\")\n",
        "                    print(f\"Done = {done}\")\n",
        "\n",
        "                    print(f\"Number of available actions = {env.action_space.n}\")\n",
        "                    print(f\"Observation space: \\n min = {env.observation_space.low} \\n max = {env.observation_space.high}\")\n",
        "\n",
        "\n",
        "The observation space (position and velocity) is continuous. We need to discretize the space in order to build a Q table.\n",
        "\n",
        "                    # Discretize the observation space to 20 bins\n",
        "                    num_position = 20\n",
        "                    num_velocity = 20 \n",
        "                    pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num_position)\n",
        "                    vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num_velocity)\n",
        "\n",
        "                    print(f\"Discretized position space: {len(pos_space)}\\n{pos_space}\")\n",
        "                    print(f\"Discretized velocity space: {len(vel_space)}\\n{vel_space}\")\n",
        "\n",
        "                    observations = []\n",
        "                    for pos in range(num_position):\n",
        "                      for vel in range(num_velocity):\n",
        "                        observations.append((pos, vel))\n",
        "\n",
        "                    # Q table is implemented as a dictionary where key = [observation, action], value = Q value\n",
        "                    q_table = {}\n",
        "                    for state in observations:\n",
        "                      for action in [0, 1, 2]:\n",
        "                        q_table[state, action] = 0\n",
        "\n",
        "                    print(f\"Total observations: {len(observations)}\")\n",
        "                    print(f\"Total elements of the Q table: {len(q_table)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Q table looks like the following.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153719045-4da1d0a1-25ba-4b69-94e9-b0ee67a288fa.jpg\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 4: Q table with random values.</em>\n",
        "</p>"
      ],
      "metadata": {
        "id": "mSVPNQeYgEnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Next, let's implement two helper functions. \n",
        "\n",
        "__make_discrete__: convert continuous observation (position and velocity) into their discrete bins.  \n",
        "\n",
        "__get_greedy_action__: select the best action using the Q table learned so far. \n",
        "\n",
        "                    def make_discrete(observation):\n",
        "                      pos, vel =  observation\n",
        "                      pos_bin = int(np.digitize(pos, pos_space))\n",
        "                      vel_bin = int(np.digitize(vel, vel_space))\n",
        "                      return (pos_bin, vel_bin)\n",
        "\n",
        "                    def get_greedy_action(q_table, observation, actions=[0, 1, 2]):\n",
        "                      values = np.array([q_table[observation,a] for a in actions])\n",
        "                      greedy_action = np.argmax(values)\n",
        "                      return greedy_action\n"
      ],
      "metadata": {
        "id": "0AhHKyv0WMDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Exercise 1\n",
        "\n",
        "As the first exercise, let's implement the $\\epsilon$-greedy strategy for choosing an action. \n",
        "\n",
        "Please complete __select_action__ in the following.\n",
        "\n",
        "                    def select_action(current_exploration_rate, greedy_action):\n",
        "                      rand_num = ######## YOUR CODE HERE (1) #########\n",
        "\n",
        "                      if rand_num < current_exploration_rate:\n",
        "                        action = ######## YOUR CODE HERE (2) #########\n",
        "                      else: \n",
        "                        action = greedy_action\n",
        "\n",
        "                      return action \n",
        "\n",
        "Hints \n",
        "  - Sample a random number between 0 and 1 with uniform probability\n",
        "  - Randomly choose an action (`0`, `1` or `2`) with equal probability\n",
        "\n",
        "References \n",
        "  - random.random ([link](https://www.w3schools.com/python/ref_random_random.asp))\n",
        "  - random.choice ([link](https://www.w3schools.com/python/ref_random_choice.asp))"
      ],
      "metadata": {
        "id": "YbhP57uMWLXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will exponentially decay $\\epsilon$ to influence the exploration of the RL agent.\n",
        "\n",
        "$$\\text{For episode n:}$$\n",
        "$$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\text{current exploration probability(%)}~=max[~ϵ_{min}, ~ϵ_{start}(ϵ_{decay~rate})^n]. $$\n",
        "\n",
        "We will start with 100% exploration, then gradually decrease it to 5% (the minimum value).\n",
        "\n",
        "                  eps_start = 1.0\n",
        "                  eps_decay_rate = 0.95\n",
        "                  eps_min = 0.05\n"
      ],
      "metadata": {
        "id": "G09nZoCygYVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Exercise 2: \n",
        "\n",
        "Next, let's implement the Q-learning algorithm (shown in Figure 5) by first setting some parameters. \n",
        "\n",
        "                  env._max_episode_steps = 1000\n",
        "                  max_episodes = 50000\n",
        "                  learning_rate = 0.1\n",
        "                  gamma = 0.99\n",
        "                \n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/153653226-b68e09a6-b8db-48f1-80cf-42ad7425dd76.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 5: The Q-Learning algorithm.</em>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "As the second exercise, please implement the following equation. \n",
        "\n",
        "$$Q(S,A) ← Q(S,A) + \\alpha[ R + \\gamma Q(S', a) - Q(S,A)]$$\n",
        "\n",
        "                    current_exploration_rate = eps_start\n",
        "                    reward_collected = []\n",
        "                    start = time.time()\n",
        "\n",
        "                    for i in range(max_episodes):\n",
        "                      timestep = 0 \n",
        "                      score = 0\n",
        "                      done = False\n",
        "                      current_obs = env.reset()\n",
        "                      current_obs = make_discrete(current_obs)\n",
        "                      \n",
        "                      while not done:\n",
        "                        greedy_action = get_greedy_action(q_table, current_obs)\n",
        "\n",
        "                        action = select_action (current_exploration_rate, greedy_action)\n",
        "                        obs_next, reward, done, info = env.step(action)\n",
        "                        obs_next = make_discrete(obs_next)\n",
        "\n",
        "                        action_next = get_greedy_action(q_table, obs_next)\n",
        "\n",
        "                        q_table[current_obs, action] = ########### YOUR CODE HERE ##########\n",
        "                        timestep += 1\n",
        "                        score += reward\n",
        "                        current_obs = obs_next\n",
        "                        \n",
        "                      if i % 100 == 0:\n",
        "                        print(f\"Episode {i}: Exploration rate = {round(current_exploration_rate,3)}, completed with {timestep} timesteps, score = {score}\")\n",
        "                        reward_collected.append(score)\n",
        "                      \n",
        "                      current_exploration_rate = max(eps_min, current_exploration_rate*eps_decay_rate)\n",
        "      \n",
        "                    print(f\"total training time = {time.time() - start} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IDyQzX9bZBN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the reward. An example plot is shown in Figure 6. \n",
        "\n",
        "                    fig, ax = plt.subplots(figsize = (10,5))\n",
        "                    x = len(reward_collected)\n",
        "                    ax.plot(range(x), reward_collected)\n",
        "                    ax.set_title(f\"Reward Collected over {x*100} episodes\")\n",
        "                    ax.set_xlabel(\"Episodes\")\n",
        "                    ax.set_ylabel(\"Reward per episode\")\n",
        "\n",
        "                    x_labels = [f\"{int(i/1000)}k\" for  i in range(0, 100*x + 1, 1000)]\n",
        "                    ax.set_xticklabels(x_labels);\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/163268608-5452d1b1-069f-4741-86a1-2b5969cdd1c5.png\")\n",
        "\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 6: Example reward plot: the RL agent eventually learned to solve the control task (complete an episode) within 200 timesteps.</em>\n",
        "</p>"
      ],
      "metadata": {
        "id": "1bL8bZRLUmem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The increase in the `reward collected per episode` as the episodes progress is a good indicator that the agent is learning. \n",
        "\n",
        "Lastly, let's use the code below to render a rollout of agent performing the task.\n",
        "\n",
        "\n",
        "                    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "                    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "                    !pip install colabgymrender==1.0.2\n",
        "\n",
        "---\n",
        "\n",
        "                    from colabgymrender.recorder import Recorder \n",
        "\n",
        "                    env = gym.make(\"MountainCar-v0\")\n",
        "                    env = Recorder(env, './video')\n",
        "\n",
        "                    done = False \n",
        "                    current_observation = env.reset() \n",
        "                    \n",
        "                    while not done:\n",
        "                      action = select_action(current_exploration_rate, q_table, current_observation)\n",
        "                      observation_next, reward, done, info = env.step(action)\n",
        "                      current_observation = observation_next\n"
      ],
      "metadata": {
        "id": "91rBdn36Uz5e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvCrBVwGOY4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
