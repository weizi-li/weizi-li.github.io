{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Machine Learning Basics\n",
        "---\n",
        "\n",
        "## Exercise 1. Dataset Inspection\n",
        "\n",
        "For this assignment, you will need the packages `sklearn` ([link](https://scikit-learn.org/stable/index.html)) and `matplotlib` ([link](https://matplotlib.org/)). \n",
        "\n",
        "Please complete the following steps.\n",
        "\n",
        "\n",
        "- Load the “diabetdiaes dataset”: set `return_X_y` to `True` and store the features in `diabetes_X` and labels in `diabetes_y`.\n",
        "\n",
        "\n",
        "- Check the dimensions of `diabetes_X` and `diabetes_y`.\n",
        "\n",
        "\n",
        "- Visual inspection of a dataset is usually the first step after loading the dataset and also a crucial step prior to the actual learning process. However, here the feature space is high-dimensional, thus difficult to visualize. Let’s reduce it to 2-D using the following code.\n",
        "<br><br>\n",
        "\n",
        "                    from sklearn import decomposition\n",
        "                    pca = decomposition.PCA(n_components=2)\n",
        "                    pca.fit(diabetes_X)\n",
        "                    X_2d = pca.transform(diabetes_X)\n",
        "\n",
        "Tip: the first principal component can be accessed as `X_2d[:,0]`; the second principal component can be accessed as `X_2d[:,1]`."
      ],
      "metadata": {
        "id": "rtYaqij_6s2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use `matplotlib` to plot `X_2d` and produce Figure 1. Remember to provide the axis labels as appeared in the figure.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/151662229-62615e47-38f6-4024-9396-47e97b11211d.png\"/>\n",
        "\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 1: Visual inspection of the dataset.</em>\n",
        "</p>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_12vTMCo84BR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The default labels of this dataset (i.e., `diabetes_y`) are numerical and not categorical. This means if we want to perform classification instead of regression, we need to convert the labels from numerical to categorical. This kind of conversion is commonly used in various machine learning projects. \n",
        "\n",
        "Here is a conversion method: take the average value of `diabetes_y` and create another variable `y_binary` for storing the binary labels `1` and `−1`; for elements in `diabetes_y` that are greater than the average, assign them `1` otherwise `−1`.\n",
        "\n",
        "Let's visualize the dataset again but this time with the positive examples marked as `+` and negative examples marked as `x`. If successful, you should see Figure 2. Remember to provide the axis labels and legends as appeared in the figure. \n",
        "\n",
        "Tip: use `train_test_split` by importing it from `sklearn.model_selection`.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/151720700-525f60a5-a9f4-4496-9c00-a96152d023c2.png\" />\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 2: Visual inspection of the dataset with binary labels.</em>\n",
        "</p>"
      ],
      "metadata": {
        "id": "-k2a84D8AeNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2. Bias-variance Tradeoff\n",
        "\n",
        "\n",
        "First, let's generate 1-D dataset using the following code.\n",
        "\n",
        "\n",
        "                    data_X = np.array([i*np.pi/180 for i in range(1,150,2)])\n",
        "                    data_y = np.sin(X) + np.random.normal(0,0.15,len(X))\n",
        "\n",
        "\n",
        "Split `data_X` and `data_y` into training set that contains 50 samples and test set that contains 25 samples. \n",
        "\n",
        "PS. if you want repeatable experiments, you need to fix the random seed.\n",
        "\n",
        "Use the following code to train a (ordinal) linear regression model.\n",
        "\n",
        "                    from sklearn.linear_model import LinearRegression\n",
        "                    lr = LinearRegression()\n",
        "                    lr.fit(X_train, y_train)\n",
        "\n",
        "The coefficients and intercept can be accessed as `lr.coef_` and `lr.intercept_`. Use them to compute the training error and test error. Plot the fitted line along with the training set and test set as shown in Figure 3 (you may not have the same plot because the dataset is generated randomly)."
      ],
      "metadata": {
        "id": "ydtcEds7QCNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://user-images.githubusercontent.com/96804013/151720757-4ea26cc2-17c0-45fb-8273-5024a242ebea.png\" />\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Figure 3: Ordinal linear regression with the training set and test set. </em>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "The objective function of ordinal linear regression is the squared errors between predicted labels and true labels:\n",
        "\n",
        "<br>\n",
        "$$J(\\theta,\\theta_0) = \\sum_{i=1}^{n}\\left(\\left(\\theta \\cdot X^{(i)}+\\theta_0\\right) - y^{(i)}\\right)^2.$$\n",
        "<br>\n",
        "\n",
        "\n",
        "Another type of linear regression, called \"ridge regression,\" has a regularization term added to the objective function:\n",
        "\n",
        "\n",
        "<br>\n",
        "$$J(\\theta,\\theta_0) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\left(\\theta \\cdot X^{(i)}+\\theta_0\\right) - y^{(i)}\\right)^2 + \\alpha\\left(\\theta^2 + \\theta_0^2\\right).$$\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "Let's create ridge regression using the following code.\n",
        "<br>\n",
        "\n",
        "                    from sklearn.linear_model import Ridge\n",
        "                    rr = Ridge(alpha=10)\n",
        "                    rr.fit(X_train, y_train)\n",
        "\n",
        "<br>\n",
        "\n",
        "Lastly, use the coefficients `rr.coef_` and intercept `rr.intercept_` to compute the training error and test error. Plot the fitted line along with the training set and test set as in Figure 3. Do this for `alpha` under 1, 100, and 1000, respectively. What are the differences? What causes the differences? And when we will have the same line as the ordinal linear regression shown in Figure 3? \n",
        "\n"
      ],
      "metadata": {
        "id": "6sE9MIHxX9Qa"
      }
    }
  ]
}